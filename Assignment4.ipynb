{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPS 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b87060785d34c61f42c2a758f71326cb",
     "grade": false,
     "grade_id": "cell-d032f2bff8ae7ea7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Due June 3, 2018 11:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd8f8c01f8a1aebf2909e99624a3c543",
     "grade": false,
     "grade_id": "cell-fd085a91544fb57f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "Consider the following constraint satisfaction problem. A graph has nodes of the following types:\n",
    "- Triangle\n",
    "- Circle\n",
    "- Square\n",
    "- Hexagon\n",
    "- Pentagon\n",
    "\n",
    "Each node has a domain of {1, 2, ..., 9}.\n",
    "\n",
    "Each node type as the following constraints on its value:\n",
    "- Triangle - The leftmost digit of the product of all of its neightbors\n",
    "- Square - The rightmost digit of of the product of all its neighbors\n",
    "- Hexagon - The leftmost digit of the sum of all its neighbors\n",
    "- Pentagon - The rightmost digit of the sum of all its neighbors\n",
    "- Circle - No contraints\n",
    "\n",
    "Complete the function defined below:\n",
    "\n",
    "```python\n",
    "def solve_csp(nodes, arcs, max_steps):\n",
    "    \"\"\"\n",
    "    This function solves the csp using the MinConflicts Search\n",
    "    Algorithm.\n",
    "\n",
    "    INPUTS:\n",
    "    nodes:      a list of letters that indicates what type of node it is,\n",
    "                the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}\n",
    "    arcs:       a list of tuples that contains two numbers, indicating the \n",
    "                IDS of the nodes the arc connects. \n",
    "    max_steps:  max number of steps to make before giving up\n",
    "\n",
    "    RETURNS: a list of values for the soltiion to the CSP where the \n",
    "             index of the value correxponds the the value for that\n",
    "             given node.\n",
    "    \"\"\"\n",
    "    node_values = []\n",
    "\n",
    "    return node_values\n",
    "```\n",
    "\n",
    "As a reminder here is the pseudo code for the Min-Conflicts search algorithm:\n",
    "\n",
    "![minconflicts](https://docs.google.com/drawings/d/e/2PACX-1vTIdOyAKDEoK6evNWQBkx9X5kl2I7GLaUkE9TdFDRqyyNFiHeFDrA-Sm7sLob2wMSzoBk_cliRhs8PY/pub?w=927&amp;h=474)\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- It's possible that you won't converge to a solution in a single run. Try a few runs to see if you get to a solution.\n",
    "- The example is to show you what a problem looks like, I will test/grade your program on different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b43c1d03ed4c4ee88edd859216f7d25a",
     "grade": true,
     "grade_id": "cell-c95dad2f1ac8adc2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def solve_csp(nodes, arcs, max_steps):\n",
    "    \"\"\"\n",
    "    This function solves the csp using the MinConflicts Search\n",
    "    Algorithm.\n",
    "\n",
    "    INPUTS:\n",
    "    nodes:      a list of letters that indicates what type of node it is,\n",
    "                the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}\n",
    "    arcs:       a list of tuples that contains two numbers, indicating the \n",
    "                IDS of the nodes the arc connects. \n",
    "    max_steps:  max number of steps to make before giving up\n",
    "\n",
    "    RETURNS: a list of values for the soltiion to the CSP where the \n",
    "             index of the value correxponds the the value for that\n",
    "             given node.\n",
    "    \"\"\"\n",
    "    node_values = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return node_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an exmaple input to test your code on. It is solveable.\n",
    "nodes = 'CHTPS'\n",
    "arcs = [(0,1), (0,2), (1,2), (1,3), (1,4), (2,3), (2,4)]\n",
    "max_steps = 1000\n",
    "solve_csp(nodes, arcs, max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b5a3fffc0a978cbe3582ae78155d159",
     "grade": false,
     "grade_id": "cell-a64a181856d55be5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "37cec1e95737276e8d5c179c3dbbad49",
     "grade": false,
     "grade_id": "cell-972dc9abc3181961",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Solve the following  MDP using both value iteration and policy iteration, you can do this by hand or programmatically, but you need to show your work in either case. \n",
    "\n",
    "There is a self-driving taxi that takes from place to place. Its goal is to make the most money possible and it makes the most money in a particular town, MoneyTown. The taxi has a tendency to take routes that take it to different towns and it costs money for the taxi to drive from place to place.  \n",
    "\n",
    "There are three states that the taxi can be in: 'In MoneyTown', 'MoneyTown Suburbs', and 'Outside MoneyTown'. There are two actions that the taxi can take in each state: drive and wait. Driving costs \\$10. When the taxi is in money town it makes \\$30, in MoneyTown Suburbs and Outside MoneyTown it only makes \\$10. The reward for the taxi is:\n",
    "\n",
    "(money made - cost) \n",
    "\n",
    "For example if the taxi is driving around in MoneyTown, the reward is \\$30-\\$10=\\$20.\n",
    "\n",
    "If the taxi is in MoneyTown and drives, then it is still MoneyTown in the next period with probability .9, and in the MoneyTown Suburbs in the next period with probability .1. If it is MoneyTown and does not drive, these probabilities are .7 and .3, respectively. If it is in the MoneyTown Suburbs and drives, then with probability .3 it is in MoneyTown in the next period, with probability .6 it is still in MoneyTown Suburbs in the next period, and with probability .1 it is in Outside MoneyTown in the next period. If it is in MoneyTown Suburbs and does not drive, then with probability 1 it is Outside MoneyTown next period. Finally, if it is in Outside MoneyTown and drives, then in the next period it is in MoneyTown with probability .6, and at the OutSide MoneyTown with probability .4. If it does not drive, then with probability 1 it is at Outside MoneyTown in the next period. \n",
    "\n",
    "1. Draw the MDP graphically\n",
    "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
    "  - When you're done you can embed it in the jupyter notebook using markdown syntax\n",
    "  - \\!\\[alt-text\\]\\(url\\)\n",
    "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
    "  \n",
    "2. Using a discount factor of .8, solve the MDP using value iteration (until the values have become reasonably stable). You should start with the values set to zero. You should show both the optimal policy and the optimal values.\n",
    "3. Using a discount factor of .8, solve the MDP using policy iteration (until you have complete convergence). You should start with the policy that never drives. Again, you should show both the optimal policy and the optimal values (and of course they should be the same as in 2...).\n",
    "4. Change the MDP in three different ways: by changing the discount factor, changing the transition probabilities for a single action from a single state, and by changing a reward for a single action at a single state. Each of these changes should be performed separately starting at the original MDP, resulting in three new MDPs (which you do not have to draw), each of which is different from the original MDP in a single way. In each case, the change should be so that the optimal policy changes, and you should state what the optimal policy becomes and give a short intuitive argument for this.\n",
    "\n",
    "\n",
    "**If you solve the problem programmatically, put your code in here. If you solve it by hand include your work here as well. You can add cells as you feel the need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
